---
title: "Bertha: Video captioning evaluation via transfer-learned human assessment"
collection: publications
permalink: /publication/bertha
excerpt: 'Evaluating video captioning systems is a challenging task as there are multiple factors to consider; for instance: the fluency of the caption, multiple actions happening in a single scene, and the human bias of what is considered important. Most metrics try to measure how similar the system generated captions are to a single or a set of human-annotated captions. This paper presents a new method based on a deep learning model to evaluate these systems. The model is based on BERT, which is a language model that has been shown to work well in multiple NLP tasks. The aim is for the model to learn to perform an evaluation similar to that of a human. To do so, we use a dataset that contains human evaluations of system generated captions. The dataset consists of the human judgments of the captions produces by the system participating in various years of the TRECVid video to text task. BERTHA obtain favourable results, outperforming the commonly used metrics in some setups.'
date: 	2022-06-22
venue: 'Proceedings of the Thirteenth Language Resources and Evaluation Conference'
paperurl: 'https://aclanthology.org/2022.lrec-1.168/'
citation: 'Luis Lebron, Yvette Graham, Kevin McGuinness, Konstantinos Kouramas, and Noel E. O’Connor. 2022. BERTHA: Video Captioning Evaluation Via Transfer-Learned Human Assessment. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 1566–1575, Marseille, France. European Language Resources Association.'
---
Evaluating video captioning systems is a challenging task as there are multiple factors to consider; for instance: the fluency of the caption, multiple actions happening in a single scene, and the human bias of what is considered important. Most metrics try to measure how similar the system generated captions are to a single or a set of human-annotated captions. This paper presents a new method based on a deep learning model to evaluate these systems. The model is based on BERT, which is a language model that has been shown to work well in multiple NLP tasks. The aim is for the model to learn to perform an evaluation similar to that of a human. To do so, we use a dataset that contains human evaluations of system generated captions. The dataset consists of the human judgments of the captions produces by the system participating in various years of the TRECVid video to text task. BERTHA obtain favourable results, outperforming the commonly used metrics in some setups.

[Download paper here](https://aclanthology.org/2022.lrec-1.168/)

Recommended citation: 

@inproceedings{lebron-etal-2022-bertha,
    title = "{BERTHA}: Video Captioning Evaluation Via Transfer-Learned Human Assessment",
    author = "Lebron, Luis  and
      Graham, Yvette  and
      McGuinness, Kevin  and
      Kouramas, Konstantinos  and
      O{'}Connor, Noel E.",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.168",
    pages = "1566--1575",
    abstract = "Evaluating video captioning systems is a challenging task as there are multiple factors to consider; for instance: the fluency of the caption, multiple actions happening in a single scene, and the human bias of what is considered important. Most metrics try to measure how similar the system generated captions are to a single or a set of human-annotated captions. This paper presents a new method based on a deep learning model to evaluate these systems. The model is based on BERT, which is a language model that has been shown to work well in multiple NLP tasks. The aim is for the model to learn to perform an evaluation similar to that of a human. To do so, we use a dataset that contains human evaluations of system generated captions. The dataset consists of the human judgments of the captions produces by the system participating in various years of the TRECVid video to text task. BERTHA obtain favourable results, outperforming the commonly used metrics in some setups.",
}
